#config example:
#Authors: Shaokai, Jan
#sample_argument:
#type || default value
#value
#print_freq  int || 10  
#resume      bool || False     *resume from checkpoint
#gpu_id      int || null      * gpu id to use
#lr          float || 0.1      *learning rate
#arch        str  || resnet18  *architecture
#save_model  str  || saved_model.pt * filename to saved model
#load_model  str  || load_model.pt  * filename to load model
#workers     int  || 16        * number of threads for loading images
#logging     bool || False     * whether to log files
#log_dir     str  || logs      * destination folder to drop log file
#smooth_eps  float|| 0.0       * smoothing rate [0.0, 1.0], set to 0.0 to disable
#alpha       float|| 0.0       * for mixup training, lambda = Beta(alpha, alpha) distribution. Set to 0.0 to disable
#                              * chosen value of alpha in Bag of Tricks is 0.2, and increase # of training epochs by 67%
#epochs      int  || 200       * number of training epochs for chosen stage
#warmup_epochs      int  || 0       * number of epochs for lr warmup
#warmup_lr      float  || 0.0001       * initial lr before warmup
#optimizer   str  || sgd       * optimizer for the chosen stage
#lr_scheduler str || default,cosine  * learning rate scheduler
#admm_epoch  int  || 20        * how many epochs required to update z and u
#rho        float || 0.001     * regularization strength
#sparsity_type str || [irregular,column,filter,bn_filter]     * sparsity type
#multi_rho   bool  || False    * whether to use multi rho in admm stage
#verbose    bool  || False     * whether to print convergence info
#masked_progressive bool || False * whether to use masked progressive

adv:
  epsilon:
    8.0    # will be divided by 255 in the adv_train.py
  num_steps:
    10
  step_size:
    2.0   # will be divided by 255 in the adv_train.py
  random_start:
    True
  loss_func:
    xent
  width_multiplier:
    16
  init_func:
    default
general:
 sparsity_type: 
  column
 print_freq:
  10 
 resume:  
  False
 gpu_id:   # 0 = weight 1, 1 = weight 10, 2 = channel 10, 3 = channel 50
  0
 arch: 
  vgg16
 workers: 
  8
 logging: 
  False
 log_dir: 
  logs
 smooth_eps:
  0.0
 alpha:
  0.0
pretrain:
 lr:
  0.01
 epochs: 
  300
 warmup_lr:
  0.01
 warmup_epochs: 
  20
 save_model: 
  vgg16_admm_no_warmup_lr_0_01.pt
 optimizer:
  sgd
 lr_scheduler: 
  default
admm:
 lr:  
  0.01
 epochs: 
  100
 save_model: 
  vgg16_admm_channel_50_svhn_HARP_fgsm_warmup.pth.tar
 load_model: 
  vgg16_hydra_svhn.pth.tar
 optimizer:
  sgd
 lr_scheduler: 
  default
 admm_epoch: 
  30
 rho: 
  0.001
 multi_rho: 
  True
 masked_progressive: 
  False
 verbose:
  False
retrain:
 lr:  
  0.01
 epochs:
  100
 warmup_lr:
  0.01
 warmup_epochs: 
  0
 save_model: 
  vgg16_retrain_channel_10_svhn_fgsm_warmup.pth.tar
 load_model:
  vgg16_admm_channel_10_svhn_fgsm_warmup.pth.tar
 optimizer: 
  sgd
 lr_scheduler:
  default
 masked_progressive: 
  False 
resnet18_adv:
#20 conv layers, use bn1x instead of convx for bn_filter pruning
#floats
 prune_ratios:
  conv1.weight:
   0
  conv2.weight:
   0.5
  conv3.weight:
   0.5635
  conv4.weight:
   0.5
  conv5.weight:
   0.59425
  conv6.weight:
   0.5
  conv7.weight:
   0.5
  conv8.weight:
   0.5
  conv9.weight:
   0.5716875
  conv10.weight:
   0.9466875
  conv11.weight:
   0.966875
  conv12.weight:
   0.95078125
  conv13.weight:
   0.8046875
  conv14.weight:
   0.9578125
  conv15.weight:
   0.9578125
  conv16.weight:
   0.9578125
  conv17.weight:
   0.95078125
  conv18.weight:
   0.9578125
  conv19.weight:
   0.94938125
  conv20.weight:
   0.9578125
  fc1.weight:
   0.9578125
vgg16: 
#20 conv layers, use bnx instead of convx for bn_filter pruning
#floats
 prune_ratios:
  conv1.weight:
   0
  conv2.weight:
   0.0469000000000000525
  conv3.weight:
   0.0999999999999999853125
  conv4.weight:
   0
  conv5.weight:
   0
  conv6.weight:
   0
  conv7.weight:
   0.03515699999999999475
  conv8.weight:
   0.01953125
  conv9.weight:
   0.5078125
  conv10.weight:
   0.58204875
  conv11.weight:
   0.60742188
  conv12.weight:
   0.73828129999999995
  conv13.weight:
   0.74609379999999995
  fc1.weight:
   0.65527350000000016
  fc2.weight:
   0.65234379999999995
  fc3.weight:
   0.55078275
wrn_28_4:
 prune_ratios:
  conv1.weight:
   0
  conv2.weight:
   0
  conv3.weight:
   0
  conv4.weight:
   0
  conv5.weight:
   0.09375
  conv6.weight:
   0
  conv7.weight:
   0
  conv8.weight:
   0.046875
  conv9.weight:
   0
  conv10.weight:
   0.0625
  conv11.weight:
   0
  conv12.weight:
   0.4296875
  conv13.weight:
   0
  conv14.weight:
   0.3125
  conv15.weight:
   0.390625
  conv16.weight:
   0.4453125
  conv17.weight:
   0.375
  conv18.weight:
   0.4765625
  conv19.weight:
   0.2890625
  conv20.weight:
   0.25
  conv21.weight:
   0.58984375
  conv22.weight:
   0.25
  conv23.weight:
   0.62890625
  conv24.weight:
   0.70703125
  conv25.weight:
   0.52734375
  conv26.weight:
   0.48046875
  conv27.weight:
   0.60546875
  conv28.weight:
   0.68359375
  fc1.weight:
   0.48046875